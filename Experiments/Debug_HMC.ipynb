{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from autograd import grad\n",
    "\n",
    "\n",
    "def hmc(log_prior, log_likelihood, num_samples, step_size, L, init, burn, thin):\n",
    "    \"\"\"\n",
    "    :param log_prior: The log of the prior on our parameters\n",
    "    :param log_likelihood: The log of the likelihood of our data under our model\n",
    "    :param num_samples: The number of samples produced\n",
    "    :param step_size: The step-size in the Leap Frog estimator\n",
    "    :param L: The number of steps in the Leap Frog Estimator\n",
    "    :param init: The initial position of the HMC\n",
    "    :param burn: Burn-in parameter\n",
    "    :param thin: Thinning parameter\n",
    "    :return: Samples of our posterior distribution using HMC. Shape: (D, (num_samples-burn)/thin)\n",
    "    Note: We imposed here the choice of mass m = 1 and a quadratic Kinetic Energy providing a Normal Gibbs Sampler\n",
    "    \"\"\"\n",
    "\n",
    "    def U(W):\n",
    "        return -1 * (log_likelihood(W).flatten() + log_prior(W).flatten())\n",
    "\n",
    "    def K(W):\n",
    "        return np.sum(W ** 2) / 2\n",
    "\n",
    "    def K_gibbs_sampler():\n",
    "        D = init.flatten().shape[0]  # Dimensionality of our problem\n",
    "        return np.random.normal(0, 1, size=(1, D))\n",
    "\n",
    "    q_current = init\n",
    "    samples = [q_current]\n",
    "    accept = 0\n",
    "\n",
    "    # gradient U wrt to q\n",
    "    grad_U = grad(U)\n",
    "    # gradient of K wrt to p\n",
    "    grad_K = grad(K)\n",
    "\n",
    "    for i in range(num_samples):\n",
    "\n",
    "        if i % 100 == 0 and i > 0:\n",
    "            print(i, ':', accept * 1. / i)\n",
    "\n",
    "        p_current = K_gibbs_sampler()  # sample a random momentum from the Gibbs distribution\n",
    "\n",
    "        # calc position and momentum after L steps (initaliate intermediate vars)\n",
    "        q_proposal = q_current.copy()\n",
    "        p_proposal = p_current.copy()\n",
    "\n",
    "        # Leap-frog\n",
    "        for j in range(L):\n",
    "            # half-step update for momentum\n",
    "            p_step_t_half = p_proposal.flatten() - (step_size / 2.) * grad_U(q_proposal.flatten())\n",
    "            # full step update for position\n",
    "            q_proposal += step_size * p_step_t_half\n",
    "            # half-step update for momentum\n",
    "            p_proposal = p_step_t_half - (step_size / 2.) * grad_U(q_proposal)\n",
    "\n",
    "        p_proposal = - p_proposal.copy()  # reverse momentum to ensure detail balance/reversibility\n",
    "\n",
    "        # accept/reject new proposed position\n",
    "        H_proposal = U(q_proposal) + K(p_proposal)\n",
    "        H_current = U(q_current) + K(p_current)\n",
    "        proposal = np.exp(H_current - H_proposal)\n",
    "\n",
    "        alpha = min(1, proposal)\n",
    "\n",
    "        if np.random.uniform() <= alpha:\n",
    "            accept += 1  # you should keep track of your acceptances\n",
    "            q_current = q_proposal.copy()\n",
    "\n",
    "        samples.append(q_current.flatten())\n",
    "        i += 1\n",
    "\n",
    "    # burn and thin\n",
    "    burn_n = round(burn * num_samples)\n",
    "    return samples[burn_n::thin]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1. / (1. + np.exp(-z))\n",
    "\n",
    "\n",
    "def HMC_Unit_test():\n",
    "    # Generate a toy dataset for classification\n",
    "    samples = 100\n",
    "    class_0 = np.random.multivariate_normal([-1, -1], 0.5 * np.eye(2), samples)\n",
    "    class_1 = np.random.multivariate_normal([1, 1], 0.5 * np.eye(2), samples)\n",
    "    x = np.vstack((class_0, class_1))\n",
    "    y = np.array([0] * 100 + [1] * 100)\n",
    "    mean = np.zeros(3)\n",
    "    cov = 10*np.eye(3)\n",
    "    D = 3\n",
    "\n",
    "    def log_likelihood(w):\n",
    "        theta = sigmoid(w[0] + np.dot(x, w[1:]))\n",
    "        theta = np.clip(theta, 1e-15, 1-1e-15)\n",
    "        loglkhd = y * np.log(theta) + (1 - y) * np.log(1 - theta)\n",
    "        return np.sum(loglkhd)\n",
    "\n",
    "    def log_normal_prior(W):\n",
    "        logprior = -0.5 * (np.log(np.linalg.det(cov)) + D * np.log(2 * np.pi))\n",
    "        logprior += -0.5 * np.dot(np.dot(W-mean, np.linalg.inv(cov)), (W-mean).T)\n",
    "        return logprior\n",
    "\n",
    "    log_prior = log_normal_prior\n",
    "    log_likelihood = log_likelihood\n",
    "    lr = LogisticRegression(C=1., penalty='l2', solver='saga', tol=0.1)\n",
    "    lr.fit(x, y)\n",
    "    position_init = np.hstack((lr.coef_.flatten(), lr.intercept_))\n",
    "    position_init = position_init.reshape((1, 3))[0]\n",
    "    samples = hmc(log_prior, log_likelihood, 1000,  1e-1, 20, position_init, 0.1, 1)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    HMC_Unit_test()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}